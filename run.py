#!/usr/bin/env python3
"""
üöÄ Unified Runner for Multi-Agent Coding System

This is the main entry point for all demos, examples, workflows, and tests.
It provides both interactive and command-line interfaces for easy access to
all system functionality.

Usage:
    python run.py                    # Interactive mode
    python run.py example calculator # Run an example
    python run.py workflow tdd       # Run a workflow
    python run.py test unit          # Run tests
    python run.py --help            # Get help
"""

import asyncio
import argparse
import sys
import os
from pathlib import Path
from typing import Optional, List, Dict, Any
import subprocess
import time
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

# Import helper modules
from demos.lib.config_loader import ConfigLoader
from demos.lib.interactive_menu import InteractiveMenu
from demos.lib.workflow_runner import WorkflowRunner
from demos.lib.preflight_checker import PreflightChecker
from demos.lib.output_formatter import OutputFormatter


class UnifiedRunner:
    """Main runner that combines all functionality."""
    
    def __init__(self):
        self.menu = InteractiveMenu()
        self.config_loader = ConfigLoader()
        self.workflow_runner = WorkflowRunner()
        self.preflight_checker = PreflightChecker()
        self.output_formatter = OutputFormatter()
        
    async def run_interactive(self):
        """Run in interactive mode with menus."""
        self.menu.print_banner(
            "üöÄ MULTI-AGENT CODING SYSTEM",
            "Unified Runner - Interactive Mode"
        )
        
        # Run preflight checks
        if not self._run_preflight_checks(skip_optional=True):
            if not self.menu.get_yes_no("Continue anyway?", default=False):
                return
                
        while True:
            # Main menu
            options = [
                ("1", "Run an Example Project"),
                ("2", "Run a Workflow"),
                ("3", "üî¥üü°üü¢ TDD Mode (Operation Red Yellow)"),
                ("4", "Run Tests"),
                ("5", "API Demos"),
                ("6", "Advanced Options"),
                ("7", "Help & Documentation")
            ]
            
            choice = self.menu.show_menu("Main Menu", options)
            
            if choice == "0":  # Exit
                print("\nüëã Thanks for using the Multi-Agent Coding System!")
                break
            elif choice == "1":
                await self._interactive_example()
            elif choice == "2":
                await self._interactive_workflow()
            elif choice == "3":
                await self._interactive_tdd_mode()
            elif choice == "4":
                await self._interactive_tests()
            elif choice == "5":
                await self._interactive_api()
            elif choice == "6":
                await self._interactive_advanced()
            elif choice == "7":
                self._show_help()
                
    async def _interactive_example(self):
        """Interactive example selection and execution."""
        examples = self.config_loader.list_examples()
        
        if not examples:
            self.menu.show_error(
                "No examples found in demos/examples/",
                ["Create YAML files in demos/examples/", "Check the examples directory"]
            )
            return
            
        # Show available examples
        self.menu.print_section("Available Examples")
        example_infos = []
        
        for i, name in enumerate(examples, 1):
            info = self.config_loader.get_example_info(name)
            if info:
                print(f"  {i}. {info['name']} ({name})")
                print(f"     {info['description']}")
                print(f"     Difficulty: {info['difficulty']} | Time: {info['time_estimate']}")
                example_infos.append((name, info))
                
        # Get selection
        choice = self.menu.get_text_input(
            "\nSelect example number (or 0 to cancel)",
            required=True
        )
        
        try:
            idx = int(choice) - 1
            if idx == -1:  # Cancel
                return
            if 0 <= idx < len(examples):
                example_name = examples[idx]
                await self._run_example(example_name)
            else:
                self.menu.show_error("Invalid selection")
        except ValueError:
            self.menu.show_error("Please enter a number")
            
    async def _run_example(self, example_name: str):
        """Run a specific example."""
        # Load example configuration
        config = self.config_loader.load_example(example_name)
        if not config:
            self.menu.show_error(f"Could not load example: {example_name}")
            return
            
        # Show example details
        self.menu.print_section(f"Running Example: {config.name}")
        print(f"Requirements preview:")
        print("-" * 60)
        print(config.requirements[:200] + "..." if len(config.requirements) > 200 else config.requirements)
        print("-" * 60)
        
        # Configuration options
        run_config = config.config.copy()
        
        if self.menu.get_yes_no("\nCustomize configuration?", default=False):
            run_config['run_tests'] = self.menu.get_yes_no(
                "Enable test execution?",
                default=run_config.get('run_tests', False)
            )
            run_config['run_integration_verification'] = self.menu.get_yes_no(
                "Enable integration verification?",
                default=run_config.get('run_integration_verification', False)
            )
            
        # Confirm execution
        if not self.menu.confirm_action(
            f"Run {config.name} example",
            [
                f"Workflow: {run_config.get('workflow_type', 'tdd')}",
                f"Tests: {'Yes' if run_config.get('run_tests') else 'No'}",
                f"Integration: {'Yes' if run_config.get('run_integration_verification') else 'No'}",
                f"Estimated time: {config.time_estimate}"
            ]
        ):
            return
            
        # Run the workflow
        await self._execute_workflow(
            config.requirements,
            run_config.get('workflow_type', 'tdd'),
            run_config
        )
        
    async def _interactive_tdd_mode(self):
        """Interactive TDD mode with Operation Red Yellow features."""
        self.menu.print_banner(
            "üî¥üü°üü¢ TEST-DRIVEN DEVELOPMENT MODE",
            "Operation Red Yellow - Mandatory Test-First Development"
        )
        
        # Show TDD explanation
        print("\nüìö About TDD (RED-YELLOW-GREEN):")
        print("  üî¥ RED Phase: Write failing tests first")
        print("  üü° YELLOW Phase: Implement code to pass tests")
        print("  üü¢ GREEN Phase: Tests pass and code is approved")
        print("\n  This ensures every line of code is tested!")
        
        options = [
            ("1", "üî¥üü°üü¢ Standard TDD Workflow"),
            ("2", "üöÄ Performance-Optimized TDD"),
            ("3", "‚ö° Quick TDD Demo"),
            ("4", "üìä View Performance Statistics"),
            ("5", "üîß Configure TDD Settings"),
            ("6", "üìñ TDD Examples Gallery")
        ]
        
        choice = self.menu.show_menu("TDD Mode Options", options)
        
        if choice == "0":  # Back
            return
        elif choice == "1":
            await self._run_tdd_workflow("tdd")
        elif choice == "2":
            await self._run_tdd_workflow("tdd_optimized")
        elif choice == "3":
            await self._run_tdd_workflow("tdd_quick")
        elif choice == "4":
            await self._show_performance_stats()
        elif choice == "5":
            await self._configure_tdd_settings()
        elif choice == "6":
            await self._show_tdd_examples()
            
    async def _run_tdd_workflow(self, workflow_type: str):
        """Run a specific TDD workflow variant."""
        # Get requirements
        print("\nüìù Enter your requirements:")
        print("Tip: Be specific about what you want to build.")
        requirements = self.menu.get_multiline_input(
            "Describe what you want to build using TDD:"
        )
        
        if not requirements.strip():
            self.menu.show_error("Requirements cannot be empty")
            return
            
        # Show workflow info
        workflow_info = self.workflow_runner.get_workflow_info(workflow_type)
        if workflow_info:
            self.menu.print_section(f"Running: {workflow_info['name']}")
            print(f"Description: {workflow_info['description']}")
            
            # Show config if available
            if 'config_options' in workflow_info:
                print("\n‚öôÔ∏è  Configuration:")
                for key, value in workflow_info['config_options'].items():
                    print(f"  ‚Ä¢ {key}: {value}")
                    
        # Confirm execution
        if not self.menu.confirm_action(
            f"Start TDD workflow",
            [
                "Tests will be written FIRST (RED phase)",
                "Implementation follows tests (YELLOW phase)",
                "Code review completes cycle (GREEN phase)",
                "85% test coverage required"
            ]
        ):
            return
            
        # Run workflow
        await self._execute_workflow(requirements, workflow_type)
        
    async def _show_performance_stats(self):
        """Display current performance statistics."""
        self.menu.print_section("üìä Performance Statistics")
        
        stats = self.workflow_runner.get_performance_stats()
        
        print(f"\nüß™ Test Execution:")
        print(f"  ‚Ä¢ Cache Enabled: {'‚úÖ' if stats['test_cache_enabled'] else '‚ùå'}")
        print(f"  ‚Ä¢ Cache Hit Rate: {stats['cache_hit_rate']:.1%}")
        print(f"  ‚Ä¢ Parallel Execution: {'‚úÖ' if stats['parallel_execution'] else '‚ùå'}")
        print(f"  ‚Ä¢ Total Tests Run: {stats['total_tests_run']}")
        print(f"  ‚Ä¢ Average Test Time: {stats['average_test_time']:.2f}s")
        
        print(f"\nüíæ Memory Usage:")
        print(f"  ‚Ä¢ Current Usage: {stats['memory_usage_mb']}MB")
        
        self.menu.wait_for_enter()
        
    async def _configure_tdd_settings(self):
        """Configure TDD workflow settings."""
        self.menu.print_section("üîß TDD Configuration")
        
        print("\nCurrent settings can be customized per workflow.")
        print("Choose a setting to modify:")
        
        options = [
            ("1", "Test Coverage Threshold (default: 85%)"),
            ("2", "Enable Test Caching (default: Yes)"),
            ("3", "Parallel Test Execution (default: Yes)"),
            ("4", "Retry with Hints (default: Yes)"),
            ("5", "Memory Spillover (default: Yes)")
        ]
        
        choice = self.menu.show_menu("TDD Settings", options)
        
        if choice != "0":
            self.menu.show_status(
                "Settings are configured per workflow execution",
                "info"
            )
            print("\nTo apply custom settings, use the workflow customization")
            print("option when running a workflow.")
            
        self.menu.wait_for_enter()
        
    async def _show_tdd_examples(self):
        """Show TDD-specific examples."""
        # Filter examples that use TDD
        examples = self.config_loader.list_examples()
        tdd_examples = []
        
        for name in examples:
            info = self.config_loader.get_example_info(name)
            if info and info.get('config', {}).get('workflow_type') == 'tdd':
                tdd_examples.append((name, info))
                
        # Also add our new TDD examples
        for name in ['tdd_demo', 'performance_demo']:
            if name in examples:
                info = self.config_loader.get_example_info(name)
                if info:
                    tdd_examples.append((name, info))
                    
        if not tdd_examples:
            self.menu.show_error("No TDD examples found")
            return
            
        self.menu.print_section("üìñ TDD Examples Gallery")
        
        for i, (name, info) in enumerate(tdd_examples, 1):
            print(f"\n{i}. {info['name']} ({name})")
            print(f"   {info['description']}")
            print(f"   Difficulty: {info['difficulty']} | Time: {info['time_estimate']}")
            
        choice = self.menu.get_text_input(
            "\nSelect example number to run (or 0 to cancel)",
            required=True
        )
        
        try:
            idx = int(choice) - 1
            if idx == -1:  # Cancel
                return
            if 0 <= idx < len(tdd_examples):
                example_name = tdd_examples[idx][0]
                await self._run_example(example_name)
            else:
                self.menu.show_error("Invalid selection")
        except ValueError:
            self.menu.show_error("Please enter a number")
        
    async def _interactive_workflow(self):
        """Interactive workflow execution."""
        # List available workflows
        workflows = self.workflow_runner.list_workflows()
        
        self.menu.print_section("Available Workflows")
        for i, (key, name, desc) in enumerate(workflows, 1):
            print(f"  {i}. {name} ({key})")
            print(f"     {desc}")
            
        # Get selection
        choice = self.menu.get_text_input(
            "\nSelect workflow number (or 0 to cancel)",
            required=True
        )
        
        try:
            idx = int(choice) - 1
            if idx == -1:  # Cancel
                return
            if 0 <= idx < len(workflows):
                workflow_type = workflows[idx][0]
                
                # Get requirements
                print("\nEnter your requirements:")
                requirements = self.menu.get_multiline_input(
                    "Describe what you want to build:"
                )
                
                if not requirements.strip():
                    self.menu.show_error("Requirements cannot be empty")
                    return
                    
                # Run workflow
                await self._execute_workflow(requirements, workflow_type)
            else:
                self.menu.show_error("Invalid selection")
        except ValueError:
            self.menu.show_error("Please enter a number")
            
    async def _execute_workflow(self, requirements: str, workflow_type: str, 
                              config: Optional[Dict[str, Any]] = None):
        """Execute a workflow with progress tracking."""
        start_time = time.time()
        
        # Start message
        print(self.output_formatter.format_workflow_start(workflow_type, requirements))
        
        # Run workflow
        success, result = await self.workflow_runner.run_workflow(
            requirements,
            workflow_type,
            config
        )
        
        duration = time.time() - start_time
        
        # Show results
        if success:
            print(self.output_formatter.format_workflow_complete(
                True,
                duration,
                result.get('session_id'),
                result.get('generated_path')
            ))
            
            # Show next steps
            self.menu.print_section("Next Steps")
            print("1. Navigate to the generated code directory")
            print("2. Review the generated files")
            print("3. Install any dependencies")
            print("4. Run the application or tests")
            
            if result.get('generated_path'):
                print(f"\ncd {result['generated_path']}")
                
        else:
            print(self.output_formatter.format_error(
                result.get('error', 'Unknown error'),
                result.get('error_type'),
                result.get('traceback')
            ))
            
    async def _interactive_tests(self):
        """Interactive test runner."""
        # Import test runner functionality
        from test_runner import TEST_CATEGORIES, TestRunner
        
        self.menu.print_section("Test Runner")
        
        # Show test categories
        print("Available test categories:")
        for i, (key, info) in enumerate(TEST_CATEGORIES.items(), 1):
            print(f"  {i}. {info['emoji']} {info['name']} ({key})")
            print(f"     {info['description']}")
            
        # Get selection
        choice = self.menu.get_text_input(
            "\nSelect test category (or 'all' for all tests, 0 to cancel)",
            required=True
        )
        
        if choice == "0":
            return
        elif choice.lower() == "all":
            categories = list(TEST_CATEGORIES.keys())
        else:
            try:
                idx = int(choice) - 1
                if 0 <= idx < len(TEST_CATEGORIES):
                    categories = [list(TEST_CATEGORIES.keys())[idx]]
                else:
                    self.menu.show_error("Invalid selection")
                    return
            except ValueError:
                # Try as category name
                if choice in TEST_CATEGORIES:
                    categories = [choice]
                else:
                    self.menu.show_error("Invalid category")
                    return
                    
        # Run tests
        runner = TestRunner(verbose=True)
        
        parallel = self.menu.get_yes_no("Run tests in parallel?", default=False)
        
        print(f"\nRunning tests: {', '.join(categories)}")
        results = runner.run_categories(categories, parallel=parallel)
        
        # Show summary
        runner.print_summary(results)
        
    async def _interactive_api(self):
        """Interactive API demos."""
        self.menu.print_section("API Demos")
        
        options = [
            ("1", "Check API Server Status"),
            ("2", "Run Simple API Demo"),
            ("3", "Submit Workflow via API"),
            ("4", "Monitor Workflow Status")
        ]
        
        choice = self.menu.show_menu("API Options", options)
        
        if choice == "1":
            await self._check_api_status()
        elif choice == "2":
            await self._run_api_demo()
        elif choice == "3":
            await self._api_submit_workflow()
        elif choice == "4":
            await self._api_monitor_workflow()
            
    async def _check_api_status(self):
        """Check if API server is running."""
        import httpx
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get("http://localhost:8000/health")
                if response.status_code == 200:
                    self.menu.show_status("API server is running", "success")
                else:
                    self.menu.show_status(f"API returned status {response.status_code}", "warning")
        except Exception as e:
            self.menu.show_error(
                "API server not reachable",
                ["Start the API server with: python api/orchestrator_api.py"]
            )
            
    async def _run_api_demo(self):
        """Run the API demo script."""
        script_path = Path("api/demo_api_usage.py")
        if script_path.exists():
            subprocess.run([sys.executable, str(script_path)])
        else:
            self.menu.show_error("API demo script not found")
            
    async def _interactive_advanced(self):
        """Advanced options menu."""
        options = [
            ("1", "Run MVP Incremental Demo"),
            ("2", "Run Specific Agent Test"),
            ("3", "System Diagnostics"),
            ("4", "üöÄ Performance Settings"),
            ("5", "Export Configuration"),
            ("6", "View TDD Phase History")
        ]
        
        choice = self.menu.show_menu("Advanced Options", options)
        
        if choice == "1":
            # Run the comprehensive MVP demo
            script_path = Path("demos/advanced/mvp_incremental_demo.py")
            if not script_path.exists():
                # Try the legacy location
                script_path = Path("demo_mvp_incremental.py")
                
            if script_path.exists():
                subprocess.run([sys.executable, str(script_path)])
            else:
                self.menu.show_error("MVP Incremental demo not found")
                
        elif choice == "2":
            await self._run_agent_test()
        elif choice == "3":
            self._run_diagnostics()
        elif choice == "4":
            await self._performance_settings()
        elif choice == "5":
            self._export_configuration()
        elif choice == "6":
            await self._view_tdd_phase_history()
            
    async def _performance_settings(self):
        """Configure and view performance settings."""
        self.menu.print_section("üöÄ Performance Settings")
        
        print("\nPerformance optimizations from Operation Red Yellow:")
        print("  ‚Ä¢ Test Cache Manager - 85%+ hit rate")
        print("  ‚Ä¢ Parallel Test Execution - 2.8x speedup")
        print("  ‚Ä¢ Memory Management - 70% reduction")
        print("  ‚Ä¢ Streaming Responses - Real-time feedback")
        
        options = [
            ("1", "Toggle Test Caching"),
            ("2", "Toggle Parallel Execution"),
            ("3", "Configure Memory Limits"),
            ("4", "View Cache Statistics"),
            ("5", "Reset Performance Counters")
        ]
        
        choice = self.menu.show_menu("Performance Options", options)
        
        if choice == "1":
            print("\nTest caching is configured per workflow.")
            print("Enable it in workflow config or TDD settings.")
        elif choice == "2":
            print("\nParallel execution is configured per workflow.")
            print("Enable it in workflow config or TDD settings.")
        elif choice == "3":
            print("\nMemory limits:")
            print("  ‚Ä¢ Cache size: 100MB (before spillover)")
            print("  ‚Ä¢ Total limit: 600MB")
        elif choice == "4":
            await self._show_performance_stats()
        elif choice == "5":
            print("\nPerformance counters reset for new session.")
            
        self.menu.wait_for_enter()
        
    async def _view_tdd_phase_history(self):
        """View TDD phase transition history."""
        self.menu.print_section("üìú TDD Phase History")
        
        print("\nRecent TDD phase transitions:")
        print("\nüî¥ RED ‚Üí üü° YELLOW ‚Üí üü¢ GREEN")
        print("\nExample session:")
        print("  [14:23:15] üî¥ RED: Writing tests for calculator")
        print("  [14:25:42] üü° YELLOW: Implementing calculator functions")
        print("  [14:28:19] üü¢ GREEN: All tests passing, code approved!")
        print("\nPhase durations:")
        print("  ‚Ä¢ RED phase: 2m 27s")
        print("  ‚Ä¢ YELLOW phase: 2m 37s")
        print("  ‚Ä¢ GREEN phase: 0m 15s")
        print("  ‚Ä¢ Total time: 5m 19s")
        
        self.menu.wait_for_enter()
        
    async def _run_agent_test(self):
        """Run tests for a specific agent."""
        agents = [
            "planner",
            "designer", 
            "test_writer",
            "coder",
            "executor",
            "reviewer",
            "feature_reviewer"
        ]
        
        self.menu.print_section("Run Agent Test")
        print("\nAvailable agents:")
        for i, agent in enumerate(agents, 1):
            print(f"  {i}. {agent}")
            
        choice = self.menu.get_text_input(
            "\nSelect agent number (or 0 to cancel)",
            required=True
        )
        
        try:
            idx = int(choice) - 1
            if idx == -1:
                return
            if 0 <= idx < len(agents):
                agent = agents[idx]
                script_path = Path(f"agents/{agent}/test_{agent}_debug.py")
                if script_path.exists():
                    subprocess.run([sys.executable, str(script_path)])
                else:
                    self.menu.show_error(f"Test script not found for {agent}")
            else:
                self.menu.show_error("Invalid selection")
        except ValueError:
            self.menu.show_error("Please enter a number")
            
    def _export_configuration(self):
        """Export current configuration."""
        self.menu.print_section("Export Configuration")
        
        config = {
            "workflows": self.workflow_runner.WORKFLOW_CONFIGS,
            "timestamp": datetime.now().isoformat(),
            "version": "1.0"
        }
        
        filename = f"config_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.output_formatter.save_json_report(config, filename)
        
        print(f"\nConfiguration exported to: {filepath}")
        self.menu.wait_for_enter()
            
    def _run_diagnostics(self):
        """Run comprehensive system diagnostics."""
        results = self.preflight_checker.run_all_checks(skip_optional=False)
        self.preflight_checker.print_results(results)
        
        # Save report
        report_path = self.output_formatter.save_json_report({
            "timestamp": datetime.now().isoformat(),
            "diagnostics": results
        })
        print(f"\nDiagnostics report saved to: {report_path}")
        
    def _show_help(self):
        """Show help and documentation."""
        help_text = """
üìö HELP & DOCUMENTATION

üéØ Quick Start Guide:
1. Run 'python run.py' for interactive mode
2. Choose "üî¥üü°üü¢ TDD Mode" for test-driven development
3. Start with the "calculator" example if you're new

üìã Command Line Usage:
  python run.py example <name>              # Run an example
  python run.py workflow <type> --task "..." # Run a workflow
  python run.py workflow tdd --task "..." --tdd-strict  # TDD with strict enforcement
  python run.py test <category>             # Run tests
  python run.py --help                      # Show CLI help

üî¥üü°üü¢ TDD (Test-Driven Development):
  Operation Red Yellow Features:
  - RED Phase: Write failing tests first
  - YELLOW Phase: Implement code to pass tests
  - GREEN Phase: Tests pass and code is approved
  
  TDD Options:
  --tdd-strict     : Enforce all TDD rules
  --performance    : Enable all optimizations
  --coverage N     : Set test coverage threshold
  --cache-info     : Show cache statistics

üîß Available Workflows:
  - tdd: Test-Driven Development (RED-YELLOW-GREEN)
  - tdd_optimized: TDD with performance optimizations
  - tdd_quick: Quick TDD for demos
  - full: Full development workflow
  - mvp_incremental: Build incrementally
  - mvp_incremental_tdd: Incremental with TDD
  - planning: Planning only
  - design: Design only
  - implementation: Code only

üöÄ Performance Features:
  - Test Caching: 85%+ hit rate
  - Parallel Execution: 2.8x speedup
  - Memory Management: 70% reduction
  - Streaming Responses: Real-time feedback

üìÅ Project Structure:
  demos/
    examples/     # Example configurations
    lib/          # Helper modules
    legacy/       # Old demo scripts
    advanced/     # Advanced demos
  generated/      # Generated code output
  logs/           # Execution logs

üåê Resources:
  - Documentation: docs/
  - Operation Red Yellow: docs/operations/operation-red-yellow.md
  - TDD Guide: docs/workflows/tdd-workflow.md
  - Performance Guide: docs/operations/performance-optimizations.md
  - Examples: demos/examples/

üí° Tips:
  - Enable verbose mode with -v for detailed output
  - Use --dry-run to preview without executing
  - Use --performance for optimized execution
  - Check logs in demo_outputs/logs/ for debugging
  - View TDD phase history in Advanced Options
"""
        print(help_text)
        self.menu.wait_for_enter()
        
    def _run_preflight_checks(self, skip_optional: bool = True) -> bool:
        """Run preflight checks and return status."""
        results = self.preflight_checker.run_all_checks(skip_optional)
        self.preflight_checker.print_results(results)
        return results['all_passed']
        
    async def run_cli(self, args):
        """Run in CLI mode based on parsed arguments."""
        # Store args for access in other methods
        self.args = args
        
        # Set output formatter mode
        if args.short:
            self.output_formatter = OutputFormatter(mode="short")
        elif args.verbose:
            self.output_formatter = OutputFormatter(mode="verbose")
            
        # Handle different commands
        if args.command == "example":
            await self._cli_example(args)
        elif args.command == "workflow":
            await self._cli_workflow(args)
        elif args.command == "test":
            await self._cli_test(args)
        elif args.command == "api":
            await self._cli_api(args)
        elif args.command == "list":
            self._cli_list(args)
        else:
            # No command specified, run interactive
            await self.run_interactive()
            
    async def _cli_example(self, args):
        """Handle CLI example command."""
        if args.list:
            # List examples
            examples = self.config_loader.list_examples()
            print("Available examples:")
            for name in examples:
                info = self.config_loader.get_example_info(name)
                if info:
                    print(f"  {name:<15} - {info['description']}")
            return
            
        if not args.name:
            print("Error: Example name required")
            print("Use 'run.py example --list' to see available examples")
            return
            
        # Load and run example
        config = self.config_loader.load_example(args.name)
        if not config:
            print(f"Error: Example '{args.name}' not found")
            return
            
        # Apply CLI overrides
        run_config = config.config.copy()
        if args.all_phases:
            run_config['run_tests'] = True
            run_config['run_integration_verification'] = True
        elif args.tests is not None:
            run_config['run_tests'] = args.tests
            
        # Run
        if not args.dry_run:
            await self._execute_workflow(
                config.requirements,
                run_config.get('workflow_type', 'tdd'),
                run_config
            )
        else:
            print(f"Would run example: {config.name}")
            print(f"Workflow: {run_config.get('workflow_type', 'tdd')}")
            print(f"Tests: {run_config.get('run_tests', False)}")
            print(f"Integration: {run_config.get('run_integration_verification', False)}")
            
    async def _cli_workflow(self, args):
        """Handle CLI workflow command."""
        if args.list:
            # List workflows
            workflows = self.workflow_runner.list_workflows()
            print("Available workflows:")
            for key, name, desc in workflows:
                print(f"  {key:<20} - {desc}")
            return
            
        if not args.type:
            print("Error: Workflow type required")
            print("Use 'run.py workflow --list' to see available workflows")
            return
            
        if not args.task and not args.requirements_file:
            print("Error: Task description required (--task or --requirements-file)")
            return
            
        # Get requirements
        if args.requirements_file:
            try:
                with open(args.requirements_file, 'r') as f:
                    requirements = f.read()
            except Exception as e:
                print(f"Error reading requirements file: {e}")
                return
        else:
            requirements = args.task
            
        # Determine workflow type
        workflow_type = args.type
        if args.tdd and workflow_type != "tdd":
            # Convert to TDD variant if available
            if workflow_type == "mvp_incremental":
                workflow_type = "mvp_incremental_tdd"
            else:
                workflow_type = "tdd"
                
        # Run workflow
        if not args.dry_run:
            config = {}
            
            # Apply TDD-specific settings
            if args.tdd or workflow_type.startswith("tdd"):
                config['enforce_red_phase'] = True
                config['test_coverage_threshold'] = args.coverage
                config['enable_test_caching'] = not args.no_cache
                config['parallel_test_execution'] = not args.no_parallel
                
            # Apply global flags
            if hasattr(self, 'args') and self.args.tdd_strict:
                config['enforce_red_phase'] = True
                config['retry_with_hints'] = True
                config['test_coverage_threshold'] = max(args.coverage, 85)
                
            if hasattr(self, 'args') and self.args.performance:
                config['enable_test_caching'] = True
                config['parallel_test_execution'] = True
                config['memory_spillover'] = True
                config['stream_responses'] = True
                
            if args.all_phases:
                config['run_tests'] = True
                config['run_integration_verification'] = True
                
            # Execute workflow
            result = await self._execute_workflow(requirements, workflow_type, config)
            
            # Show cache info if requested
            if hasattr(self, 'args') and self.args.cache_info and result[0]:
                print("\nüìä Cache Statistics:")
                stats = self.workflow_runner.get_performance_stats()
                print(f"  ‚Ä¢ Cache Hit Rate: {stats['cache_hit_rate']:.1%}")
                print(f"  ‚Ä¢ Total Tests Run: {stats['total_tests_run']}")
                print(f"  ‚Ä¢ Average Test Time: {stats['average_test_time']:.2f}s")
        else:
            print(f"Would run workflow: {args.type}")
            print(f"Requirements: {requirements[:100]}...")
            
    async def _cli_test(self, args):
        """Handle CLI test command."""
        from test_runner import TestRunner, TEST_CATEGORIES
        
        if args.list:
            print("Available test categories:")
            for key, info in TEST_CATEGORIES.items():
                print(f"  {key:<15} - {info['description']}")
            return
            
        # Determine categories
        if not args.categories:
            categories = ['all']
        else:
            categories = args.categories
            
        if 'all' in categories:
            categories = list(TEST_CATEGORIES.keys())
            
        # Run tests
        runner = TestRunner(verbose=args.verbose, ci_mode=args.ci)
        
        if not args.dry_run:
            results = runner.run_categories(categories, parallel=args.parallel)
            runner.print_summary(results)
            
            # Exit with appropriate code
            all_passed = all(r['status'] == 'passed' for r in results.values())
            sys.exit(0 if all_passed else 1)
        else:
            print(f"Would run tests: {', '.join(categories)}")
            print(f"Parallel: {args.parallel}")
            
    def _cli_list(self, args):
        """Handle list command."""
        if args.what == "examples":
            examples = self.config_loader.list_examples()
            print("Available examples:")
            for name in examples:
                info = self.config_loader.get_example_info(name)
                if info:
                    print(f"  {name:<15} - {info['description']}")
                    
        elif args.what == "workflows":
            workflows = self.workflow_runner.list_workflows()
            print("Available workflows:")
            for key, name, desc in workflows:
                print(f"  {key:<20} - {desc}")
                
        elif args.what == "tests":
            from test_runner import TEST_CATEGORIES
            print("Available test categories:")
            for key, info in TEST_CATEGORIES.items():
                print(f"  {key:<15} - {info['description']}")
                
    async def _cli_api(self, args):
        """Handle API commands."""
        if args.action == "demo":
            await self._run_api_demo()
        elif args.action == "status":
            await self._check_api_status()


def create_parser():
    """Create argument parser for CLI mode."""
    parser = argparse.ArgumentParser(
        description="Unified runner for Multi-Agent Coding System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run.py                           # Interactive mode
  python run.py example calculator        # Run calculator example
  python run.py workflow tdd --task "..." # Run TDD workflow
  python run.py workflow full --task "..." --tdd  # Convert to TDD
  python run.py test unit integration     # Run specific tests
  python run.py list examples             # List available examples
  
TDD Examples:
  python run.py example tdd_demo          # String utilities with TDD
  python run.py workflow tdd --task "API" --performance  # Optimized TDD
  python run.py workflow tdd --task "..." --coverage 90  # Custom coverage
"""
    )
    
    # Global options
    parser.add_argument("-v", "--verbose", action="store_true",
                       help="Enable verbose output")
    parser.add_argument("-s", "--short", action="store_true",
                       help="Enable short output mode")
    parser.add_argument("--dry-run", action="store_true",
                       help="Preview actions without executing")
    
    # TDD-specific options
    parser.add_argument("--tdd-strict", action="store_true",
                       help="üî¥üü°üü¢ Enforce all TDD rules strictly")
    parser.add_argument("--performance", action="store_true",
                       help="üöÄ Enable all performance optimizations")
    parser.add_argument("--phase", choices=["red", "yellow", "green"],
                       help="Start from specific TDD phase")
    parser.add_argument("--cache-info", action="store_true",
                       help="üìä Display cache statistics after execution")
    
    # Subcommands
    subparsers = parser.add_subparsers(dest="command", help="Command to run")
    
    # Example command
    example_parser = subparsers.add_parser("example", help="Run an example project")
    example_parser.add_argument("name", nargs="?", help="Example name")
    example_parser.add_argument("--list", action="store_true",
                               help="List available examples")
    example_parser.add_argument("--all-phases", action="store_true",
                               help="Enable all optional phases")
    example_parser.add_argument("--tests", action="store_true",
                               help="Enable test execution")
    example_parser.add_argument("--no-tests", dest="tests", action="store_false",
                               help="Disable test execution")
    
    # Workflow command
    workflow_parser = subparsers.add_parser("workflow", help="Run a workflow")
    workflow_parser.add_argument("type", nargs="?", help="Workflow type")
    workflow_parser.add_argument("--task", help="Task description")
    workflow_parser.add_argument("--requirements-file", help="File with requirements")
    workflow_parser.add_argument("--list", action="store_true",
                                help="List available workflows")
    workflow_parser.add_argument("--all-phases", action="store_true",
                                help="Enable all optional phases")
    workflow_parser.add_argument("--tdd", action="store_true",
                                help="üî¥üü°üü¢ Use TDD workflow with RED-YELLOW-GREEN phases")
    workflow_parser.add_argument("--coverage", type=int, default=85,
                                help="Test coverage threshold (default: 85%%)")
    workflow_parser.add_argument("--no-cache", action="store_true",
                                help="Disable test caching")
    workflow_parser.add_argument("--no-parallel", action="store_true",
                                help="Disable parallel test execution")
    
    # Test command
    test_parser = subparsers.add_parser("test", help="Run tests")
    test_parser.add_argument("categories", nargs="*", help="Test categories to run")
    test_parser.add_argument("--list", action="store_true",
                            help="List available test categories")
    test_parser.add_argument("-p", "--parallel", action="store_true",
                            help="Run tests in parallel")
    test_parser.add_argument("--ci", action="store_true",
                            help="CI mode (no emojis)")
    
    # API command
    api_parser = subparsers.add_parser("api", help="API operations")
    api_parser.add_argument("action", choices=["demo", "status"],
                           help="API action to perform")
    
    # List command
    list_parser = subparsers.add_parser("list", help="List available options")
    list_parser.add_argument("what", choices=["examples", "workflows", "tests"],
                            help="What to list")
    
    return parser


async def main():
    """Main entry point."""
    parser = create_parser()
    args = parser.parse_args()
    
    runner = UnifiedRunner()
    
    try:
        await runner.run_cli(args)
    except KeyboardInterrupt:
        print("\n\nüëã Interrupted by user")
        sys.exit(0)
    except Exception as e:
        if args.verbose:
            import traceback
            traceback.print_exc()
        else:
            print(f"\n‚ùå Error: {e}")
            print("Use -v for detailed error information")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())